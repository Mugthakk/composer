{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import LSTM, Dropout, Dense, BatchNormalization, Activation, Input, TimeDistributed\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU, PReLU\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta, Adagrad\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from assignment.helpers import datapreparation as prep\n",
    "\n",
    "\n",
    "\n",
    "fs1_dirpath = \"./assignment/datasets/training/piano_roll_fs1\"\n",
    "fs5_dirpath = \"./assignment/datasets/training/piano_roll_fs5\"\n",
    "\n",
    "\n",
    "# Load initial data\n",
    "datasets = prep.load_all_dataset(fs5_dirpath)\n",
    "dataset_names = prep.load_all_dataset_names(fs5_dirpath)\n",
    "unique_names = set()\n",
    "for name in dataset_names: # Make sure the same names get the same encoding each run\n",
    "    unique_names.add(name)\n",
    "unique_names = list(unique_names)\n",
    "name_to_int = dict([(unique_names[i], i) for i in range(len(unique_names))])\n",
    "int_to_name = dict([(i, unique_names[i]) for i in range(len(unique_names))])\n",
    "dataset_names = to_categorical([name_to_int[name] for name in dataset_names]) # one-hot encode the composers\n",
    "datasets = [dataset[:, 1:] for dataset in datasets] # Remove the headers\n",
    "\n",
    "# Setting initial parameters\n",
    "dataset_id_names = dict(zip(np.arange(len(dataset_names)), dataset_names))\n",
    "longest_song = max(datasets[i].shape[1] for i in range(len(datasets)))\n",
    "sequence_length = 20\n",
    "length = longest_song//sequence_length + 1\n",
    "num_keys = len(datasets[0])\n",
    "parts_per_song = int(longest_song/sequence_length)\n",
    "composer_encoding_len=len(dataset_names[0]) # 4 composers\n",
    "\n",
    "# Makes several datasets from this first one with differing intervals between to capture the \"gaps\" between two sequences\n",
    "# Add each subsequence of each song with differing offsets ([0:10], [1:11], [2:12], ...) to retain information.\n",
    "# Unable to implement stateful, so try to retain as much information between subsequences as possible. \n",
    "# Also a way of dataset augmentation (regularization) by increasing the size of the dataset\n",
    "\n",
    "def transpose_and_label_more(dataset_names, datasets, num_keys):\n",
    "    zs = []\n",
    "    datasets_transposed = np.array([(datasets[i].T, dataset_names[i]) for i in range(len(datasets))])\n",
    "    for song, composer in datasets_transposed:\n",
    "        for offset in range(0, sequence_length, 1):\n",
    "            for i in range(0, len(song)//sequence_length-offset):\n",
    "                x = song[offset+i*sequence_length:offset+(i+1)*sequence_length]\n",
    "                if i == len(song)//sequence_length - 1: # Add the EOF marker if last seq of song\n",
    "                    y = np.append(song[offset+i*sequence_length+1:offset+(i+1)*sequence_length], np.array([np.ones(num_keys)]), 0)\n",
    "                else:\n",
    "                    y = song[offset+i*sequence_length+1:offset+(i+1)*sequence_length+1]\n",
    "                zs.append((x, y, composer))\n",
    "    np.random.shuffle(zs)\n",
    "    xs, ys, composers = [], [], []\n",
    "    for x, y, composer in zs:\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        composers.append(composer)\n",
    "    return np.array(xs), np.array(ys), np.array(composers)\n",
    "\n",
    "train_xs, train_ys, train_composers = transpose_and_label_more(dataset_names, datasets, num_keys)\n",
    "test_xs = train_xs[int(len(train_xs)*0.8):]\n",
    "train_xs = train_xs[:int(len(train_xs)*0.8)]\n",
    "test_ys = train_ys[int(len(train_ys)*0.8):]\n",
    "train_ys = train_ys[:int(len(train_ys)*0.8)]\n",
    "test_composers = train_composers[int(len(train_composers)*0.8):]\n",
    "train_composers = train_composers[:int(len(train_composers)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "specialist_input = Input(shape=(composer_encoding_len,))\n",
    "x = Dense(64, activation=\"relu\")(specialist_input) # Change the activation function within as the sigmoid quickly saturates due to shape. It's ok as output though.\n",
    "x = Dropout(0.5)(x)\n",
    "specialist_output_c = Dense(num_keys, activation=\"relu\")(x)\n",
    "specialist_output_h = Dense(num_keys, activation=\"relu\")(x)\n",
    "#zero_input = Dropout(1)(specialist_output) # Hax to privde 0 as initial h.\n",
    "\n",
    "inputs = Input(shape=(sequence_length, num_keys))\n",
    "\n",
    "# Units = units per timestep LSTM block, i.e. output dimensionality (128 here since input and output 128 keys)\n",
    "lstm1 = LSTM(num_keys,\n",
    "               activation='relu',\n",
    "               return_sequences=True,\n",
    "               name=\"lstm1\")\n",
    "lstm1_outputs = lstm1(inputs, initial_state=[specialist_output_h, specialist_output_c]) # [h = prev output, c = memory], h should be None\n",
    "normalized1 = BatchNormalization()(lstm1_outputs)\n",
    "dense1 = Dense(num_keys, activation=\"sigmoid\")(normalized1)\n",
    "\n",
    "lstm2 = LSTM(num_keys,\n",
    "               activation='relu',\n",
    "               return_sequences=True)(dense1)\n",
    "normalized2 = BatchNormalization()(lstm2)\n",
    "dense2 = Dense(num_keys, activation=\"sigmoid\")(normalized2)\n",
    "\n",
    "outputs = TimeDistributed(Dense(num_keys, activation=\"sigmoid\"))(normalized2) # Sigmoid keeps the probabilities independent of each other, while softmax does not!\n",
    "\n",
    "model = Model([inputs, specialist_input], outputs)\n",
    "\n",
    "adam = Adam(lr=0.001, amsgrad=True) \n",
    "# Ends up in a point where gradients really small, denominator really small and then loss exploding\n",
    "# v_t is based on the gradients at the current time step, and previous v_t, thus when gradient really small as well as v_t-1\n",
    "# the update denominator (sqrt(v_t) + epsilon) is so small that explodes.\n",
    "# AMSGrad maintains the maximum of all v_t until the present time step and uses this maximum value for normalizing\n",
    "# the running average of the gradient instead of the current v_t as is done in regular Adam.\n",
    "\n",
    "#model.save(\"./models/latest.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           320         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 20, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          8320        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          8320        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 20, 128)      131584      input_8[0][0]                    \n",
      "                                                                 dense_21[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 128)      512         lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 20, 128)      16512       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 20, 128)      131584      dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 128)      512         lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 20, 128)      16512       batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 314,176\n",
      "Trainable params: 313,664\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(48456, 20, 128) (48456, 4) (48456, 20, 128)\n",
      "Train on 48456 samples, validate on 12114 samples\n",
      "Epoch 1/50\n",
      "48456/48456 [==============================] - 158s 3ms/step - loss: 0.0853 - categorical_accuracy: 0.1788 - val_loss: 0.0479 - val_categorical_accuracy: 0.2100\n",
      "Epoch 2/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0462 - categorical_accuracy: 0.2168 - val_loss: 0.0446 - val_categorical_accuracy: 0.2288\n",
      "Epoch 3/50\n",
      "48456/48456 [==============================] - 177s 4ms/step - loss: 0.0436 - categorical_accuracy: 0.2280 - val_loss: 0.0425 - val_categorical_accuracy: 0.2266\n",
      "Epoch 4/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0417 - categorical_accuracy: 0.2368 - val_loss: 0.0421 - val_categorical_accuracy: 0.2359\n",
      "Epoch 5/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0401 - categorical_accuracy: 0.2450 - val_loss: 0.0395 - val_categorical_accuracy: 0.2439\n",
      "Epoch 6/50\n",
      "48456/48456 [==============================] - 174s 4ms/step - loss: 0.0389 - categorical_accuracy: 0.2518 - val_loss: 0.0384 - val_categorical_accuracy: 0.2534\n",
      "Epoch 7/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0378 - categorical_accuracy: 0.2571 - val_loss: 0.0370 - val_categorical_accuracy: 0.2742\n",
      "Epoch 8/50\n",
      "48456/48456 [==============================] - 177s 4ms/step - loss: 0.0369 - categorical_accuracy: 0.2624 - val_loss: 0.0365 - val_categorical_accuracy: 0.2538\n",
      "Epoch 9/50\n",
      "48456/48456 [==============================] - 174s 4ms/step - loss: 0.0361 - categorical_accuracy: 0.2670 - val_loss: 0.0358 - val_categorical_accuracy: 0.2663\n",
      "Epoch 10/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0354 - categorical_accuracy: 0.2711 - val_loss: 0.0349 - val_categorical_accuracy: 0.2754\n",
      "Epoch 11/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0348 - categorical_accuracy: 0.2749 - val_loss: 0.0342 - val_categorical_accuracy: 0.2811\n",
      "Epoch 12/50\n",
      "48456/48456 [==============================] - 174s 4ms/step - loss: 0.0342 - categorical_accuracy: 0.2800 - val_loss: 0.0338 - val_categorical_accuracy: 0.2895\n",
      "Epoch 13/50\n",
      "48456/48456 [==============================] - 176s 4ms/step - loss: 0.0336 - categorical_accuracy: 0.2825 - val_loss: 0.0332 - val_categorical_accuracy: 0.2849\n",
      "Epoch 14/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0331 - categorical_accuracy: 0.2849 - val_loss: 0.0329 - val_categorical_accuracy: 0.2940\n",
      "Epoch 15/50\n",
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0327 - categorical_accuracy: 0.2879 - val_loss: 0.0325 - val_categorical_accuracy: 0.2972\n",
      "Epoch 16/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0322 - categorical_accuracy: 0.2904 - val_loss: 0.0323 - val_categorical_accuracy: 0.2767\n",
      "Epoch 17/50\n",
      "48456/48456 [==============================] - 174s 4ms/step - loss: 0.0318 - categorical_accuracy: 0.2931 - val_loss: 0.0314 - val_categorical_accuracy: 0.2922\n",
      "Epoch 18/50\n",
      "48456/48456 [==============================] - 174s 4ms/step - loss: 0.0314 - categorical_accuracy: 0.2953 - val_loss: 0.0313 - val_categorical_accuracy: 0.2899\n",
      "Epoch 19/50\n",
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0311 - categorical_accuracy: 0.2971 - val_loss: 0.0310 - val_categorical_accuracy: 0.2940\n",
      "Epoch 20/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0308 - categorical_accuracy: 0.2987 - val_loss: 0.0306 - val_categorical_accuracy: 0.3039\n",
      "Epoch 21/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0304 - categorical_accuracy: 0.3003 - val_loss: 0.0301 - val_categorical_accuracy: 0.2982\n",
      "Epoch 22/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0301 - categorical_accuracy: 0.3029 - val_loss: 0.0309 - val_categorical_accuracy: 0.3367\n",
      "Epoch 23/50\n",
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0299 - categorical_accuracy: 0.3046 - val_loss: 0.0296 - val_categorical_accuracy: 0.3194\n",
      "Epoch 24/50\n",
      "48456/48456 [==============================] - 170s 4ms/step - loss: 0.0296 - categorical_accuracy: 0.3061 - val_loss: 0.0294 - val_categorical_accuracy: 0.3056\n",
      "Epoch 25/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0294 - categorical_accuracy: 0.3081 - val_loss: 0.0292 - val_categorical_accuracy: 0.3257\n",
      "Epoch 26/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0291 - categorical_accuracy: 0.3092 - val_loss: 0.0291 - val_categorical_accuracy: 0.3243\n",
      "Epoch 27/50\n",
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0289 - categorical_accuracy: 0.3109 - val_loss: 0.0287 - val_categorical_accuracy: 0.3067\n",
      "Epoch 28/50\n",
      "48456/48456 [==============================] - 168s 3ms/step - loss: 0.0287 - categorical_accuracy: 0.3109 - val_loss: 0.0283 - val_categorical_accuracy: 0.3273\n",
      "Epoch 29/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0285 - categorical_accuracy: 0.3128 - val_loss: 0.0281 - val_categorical_accuracy: 0.3278\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0283 - categorical_accuracy: 0.3135 - val_loss: 0.0283 - val_categorical_accuracy: 0.3283\n",
      "Epoch 31/50\n",
      "48456/48456 [==============================] - 168s 3ms/step - loss: 0.0281 - categorical_accuracy: 0.3143 - val_loss: 0.0281 - val_categorical_accuracy: 0.3230\n",
      "Epoch 32/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0280 - categorical_accuracy: 0.3158 - val_loss: 0.0278 - val_categorical_accuracy: 0.3192\n",
      "Epoch 33/50\n",
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0278 - categorical_accuracy: 0.3172 - val_loss: 0.0278 - val_categorical_accuracy: 0.3200\n",
      "Epoch 34/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0277 - categorical_accuracy: 0.3175 - val_loss: 0.0274 - val_categorical_accuracy: 0.3385\n",
      "Epoch 35/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0275 - categorical_accuracy: 0.3192 - val_loss: 0.0283 - val_categorical_accuracy: 0.3353\n",
      "Epoch 36/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0274 - categorical_accuracy: 0.3203 - val_loss: 0.0275 - val_categorical_accuracy: 0.3412\n",
      "Epoch 37/50\n",
      "48456/48456 [==============================] - 176s 4ms/step - loss: 0.0273 - categorical_accuracy: 0.3199 - val_loss: 0.0275 - val_categorical_accuracy: 0.3197\n",
      "Epoch 38/50\n",
      "48456/48456 [==============================] - 172s 4ms/step - loss: 0.0272 - categorical_accuracy: 0.3217 - val_loss: 0.0274 - val_categorical_accuracy: 0.3192\n",
      "Epoch 39/50\n",
      "48456/48456 [==============================] - 170s 4ms/step - loss: 0.0270 - categorical_accuracy: 0.3223 - val_loss: 0.0271 - val_categorical_accuracy: 0.3167\n",
      "Epoch 40/50\n",
      "48456/48456 [==============================] - 175s 4ms/step - loss: 0.0268 - categorical_accuracy: 0.3224 - val_loss: 0.0272 - val_categorical_accuracy: 0.3313\n",
      "Epoch 41/50\n",
      "48456/48456 [==============================] - 174s 4ms/step - loss: 0.0267 - categorical_accuracy: 0.3232 - val_loss: 0.0264 - val_categorical_accuracy: 0.3305\n",
      "Epoch 42/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0266 - categorical_accuracy: 0.3241 - val_loss: 0.0266 - val_categorical_accuracy: 0.3123\n",
      "Epoch 43/50\n",
      "48456/48456 [==============================] - 171s 4ms/step - loss: 0.0265 - categorical_accuracy: 0.3241 - val_loss: 0.0262 - val_categorical_accuracy: 0.3178\n",
      "Epoch 44/50\n",
      "48456/48456 [==============================] - 173s 4ms/step - loss: 0.0264 - categorical_accuracy: 0.3253 - val_loss: 0.0260 - val_categorical_accuracy: 0.3250\n",
      "Epoch 45/50\n",
      "31488/48456 [==================>...........] - ETA: 57s - loss: 0.0262 - categorical_accuracy: 0.3268"
     ]
    }
   ],
   "source": [
    "# Want to penalize each output node independantly. \n",
    "# Log Loss aka multi-class multi-label as sigmoid -> binary CE, as want probs to be considered independent of each other.\n",
    "# Combo of sigmoid and crossentropy here log counteracts exp to reduce the saturation :)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam, # consider changing this one for others\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(time()))\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, verbose=0, mode=\"auto\")\n",
    "\n",
    "print(train_xs.shape, train_composers.shape, train_ys.shape)\n",
    "model.fit([train_xs, train_composers], train_ys,\n",
    "          epochs=50, # Train harder more for more things was too bad train man :(\n",
    "          batch_size=32,\n",
    "          shuffle=True, # shuffle here but not when constructing set to be able to validate later on :)\n",
    "          callbacks=[tensorboard, early_stop],\n",
    "          validation_data=([test_xs, test_composers], test_ys),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/nostate_20_offset_1_specialist.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = model.predict([train_xs, train_composers], verbose=True)\n",
    "maxes = [np.max(c) for c in a]\n",
    "plt.hist(maxes)\n",
    "plt.show()\n",
    "plt.hist(a[:,-1])\n",
    "plt.show()\n",
    "b = np.max(a[1][-1])\n",
    "plt.plot(a[1][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[0].T, fs=5)\n",
    "prep.visualize_piano_roll(train_xs[0].T, fs=5)\n",
    "plt.plot(a[100][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[100].T, fs=5)\n",
    "prep.visualize_piano_roll(train_xs[100].T, fs=5)\n",
    "plt.plot(a[200][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[200].T, fs=5)\n",
    "prep.visualize_piano_roll(train_xs[200].T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_song_from_predict(model, initial_data, composer, limit):\n",
    "    song = []\n",
    "    keep_producing = True\n",
    "    prev_data = initial_data\n",
    "    while keep_producing and len(song) < limit:\n",
    "        predictions = model.predict([np.array([prev_data]), composer])[0]\n",
    "        labels = np.zeros(predictions.shape)\n",
    "        labels[predictions>0.4] = 1 # Threshold to consider the key as active, binarized based on this\n",
    "        last_output = labels[-1]\n",
    "        keep_producing = np.sum(last_output) != len(last_output)\n",
    "        song.append(last_output)\n",
    "        prev_data = np.append(prev_data[1:], [last_output], 0)\n",
    "    return np.array(song)\n",
    "\n",
    "initial_step = 250\n",
    "steps = 10\n",
    "song1 = make_song_from_predict(model, train_xs[initial_step], np.array([[1.0, 0.0, 0.0, 0.0]]), sequence_length*steps)\n",
    "song2 = make_song_from_predict(model, train_xs[initial_step], np.array([[0.0, 1.0, 0.0, 0.0]]), sequence_length*steps)\n",
    "song3 = make_song_from_predict(model, train_xs[initial_step], np.array([[0.0, 0.0, 1.0, 0.0]]), sequence_length*steps)\n",
    "song4 = make_song_from_predict(model, train_xs[initial_step], np.array([[0.0, 0.0, 0.0, 1.0]]), sequence_length*steps)\n",
    "prep.embed_play_v1(song1.T, fs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.embed_play_v1(song2.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.embed_play_v1(song3.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.embed_play_v1(song4.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = train_xs[initial_step+1]\n",
    "print(train_composers[initial_step])\n",
    "print(int_to_name[list(train_composers[initial_step]).index(1)])\n",
    "for i in range(1, steps):\n",
    "    actual = np.append(actual, train_xs[initial_step+1+i], axis=0)\n",
    "actual = actual.T\n",
    "prep.visualize_piano_roll(song1.T, fs=5)\n",
    "prep.visualize_piano_roll(song2.T, fs=5)\n",
    "prep.visualize_piano_roll(song3.T, fs=5)\n",
    "prep.visualize_piano_roll(song4.T, fs=5)\n",
    "prep.visualize_piano_roll(actual, fs=5)\n",
    "prep.embed_play_v1(actual, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = load_model(\"./models/nostate_20_offset_1_specialist.h5f\")\n",
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "print(model.summary())\n",
    "a = model.evaluate([train_xs, train_composers], train_ys,\n",
    "          batch_size=32)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
