{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3342, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import LSTM, Dropout, Dense, BatchNormalization, Activation, Input, TimeDistributed\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU, PReLU\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta, Adagrad\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from assignment.helpers import datapreparation as prep\n",
    "\n",
    "# Seed session\n",
    "np.random.seed(123456)\n",
    "rn.seed(123456)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=0,\n",
    "                              inter_op_parallelism_threads=0)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(123456)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "##\n",
    "\n",
    "fs1_dirpath = \"./assignment/datasets/training/piano_roll_fs1\"\n",
    "fs5_dirpath = \"./assignment/datasets/training/piano_roll_fs5\"\n",
    "\n",
    "\n",
    "# Load initial data\n",
    "datasets = prep.load_all_dataset(fs5_dirpath)\n",
    "dataset_names = prep.load_all_dataset_names(fs5_dirpath)\n",
    "unique_names = set()\n",
    "for name in dataset_names: # Make sure the same names get the same encoding each run\n",
    "    unique_names.add(name)\n",
    "unique_names = list(unique_names)\n",
    "name_to_int = dict([(unique_names[i], i) for i in range(len(unique_names))])\n",
    "int_to_name = dict([(i, unique_names[i]) for i in range(len(unique_names))])\n",
    "dataset_names = to_categorical([name_to_int[name] for name in dataset_names]) # one-hot encode the composers\n",
    "datasets = [dataset[:, 1:] for dataset in datasets] # Remove the headers\n",
    "\n",
    "# Setting initial parameters\n",
    "dataset_id_names = dict(zip(np.arange(len(dataset_names)), dataset_names))\n",
    "longest_song = max(datasets[i].shape[1] for i in range(len(datasets)))\n",
    "sequence_length = 50\n",
    "length = longest_song//sequence_length + 1\n",
    "num_keys = len(datasets[0])\n",
    "parts_per_song = int(longest_song/sequence_length)\n",
    "composer_encoding_len=len(dataset_names[0]) # 4 composers\n",
    "\n",
    "# Makes several datasets from this first one with differing intervals between to capture the \"gaps\" between two sequences\n",
    "# Add each subsequence of each song with differing offsets ([0:10], [1:11], [2:12], ...) to retain information.\n",
    "# Unable to implement stateful, so try to retain as much information between subsequences as possible. \n",
    "# Also a way of dataset augmentation (regularization) by increasing the size of the dataset\n",
    "\n",
    "def transpose_and_label_more(dataset_names, datasets, num_keys):\n",
    "    zs = []\n",
    "    datasets_transposed = np.array([(datasets[i].T, dataset_names[i]) for i in range(len(datasets))])\n",
    "    for song, composer in datasets_transposed:\n",
    "        for offset in range(0, sequence_length, 10): # Consider dropping offset, and use stateful to retain this info!\n",
    "            for i in range(0, len(song)//sequence_length-offset):\n",
    "                x = song[offset+i*sequence_length:offset+(i+1)*sequence_length]\n",
    "                if i == len(song)//sequence_length - (1 + offset): # Add the EOF marker if last seq of song\n",
    "                    y = np.append(song[offset+i*sequence_length+1:offset+(i+1)*sequence_length], np.array([np.ones(num_keys)]), 0)\n",
    "                else:\n",
    "                    y = song[offset+i*sequence_length+1:offset+(i+1)*sequence_length+1]\n",
    "                zs.append((x, y, composer))\n",
    "    xs, ys, composers = [], [], []\n",
    "    for x, y, composer in zs:\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        composers.append(composer)\n",
    "    return np.array(xs), np.array(ys), np.array(composers)\n",
    "\n",
    "train_xs, train_ys, train_composers = transpose_and_label_more(dataset_names, datasets, num_keys)\n",
    "print(train_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "specialist_input = Input(shape=(composer_encoding_len,))\n",
    "x = Dense(32, activation=\"relu\")(specialist_input) # Change the activation function within as the sigmoid quickly saturates due to shape. It's ok as output though.\n",
    "specialist_output_h = Dense(num_keys, activation=\"relu\")(x)\n",
    "specialist_output_c = Dropout(0.2)(specialist_output_h)\n",
    "\n",
    "inputs = Input(shape=(sequence_length, num_keys))\n",
    "\n",
    "# Units = units per timestep LSTM block, i.e. output dimensionality (128 here since input and output 128 keys)\n",
    "lstm1 = LSTM(num_keys,\n",
    "               activation='relu',\n",
    "               return_sequences=True,\n",
    "               name=\"lstm1\")\n",
    "lstm1_outputs = lstm1(inputs, initial_state=[specialist_output_h, specialist_output_c]) # [h = prev output, c = memory], h should be None\n",
    "\n",
    "normalized1 = BatchNormalization()(lstm1_outputs)\n",
    "dense1 = Dense(num_keys*2, activation=\"relu\")(normalized1)\n",
    "dense1 = Dropout(0.3)(dense1)\n",
    "\n",
    "lstm2 = LSTM(num_keys,\n",
    "               activation='relu',\n",
    "               return_sequences=True)(dense1)\n",
    "\n",
    "normalized2 = BatchNormalization()(lstm2)\n",
    "outputs = TimeDistributed(Dense(num_keys, activation=\"sigmoid\"))(normalized2) \n",
    "# Sigmoid keeps the probabilities independent of each other, while softmax does not!\n",
    "# kernel_regularizer = regulizer on weights\n",
    "# activity_regularizer = regularizer on outputs aka activations\n",
    "\n",
    "model = Model([inputs, specialist_input], outputs)\n",
    "\n",
    "adam = Adam(lr=0.01, amsgrad=True) \n",
    "# Ends up in a point where gradients really small, denominator really small and then loss exploding\n",
    "# v_t is based on the gradients at the current time step, and previous v_t, thus when gradient really small as well as v_t-1\n",
    "# the update denominator (sqrt(v_t) + epsilon) is so small that explodes.\n",
    "# AMSGrad maintains the maximum of all v_t until the present time step and uses this maximum value for normalizing\n",
    "# the running average of the gradient instead of the current v_t as is done in regular Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          4224        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 50, 128)      131584      input_2[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 50, 128)      512         lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 50, 256)      33024       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50, 256)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50, 128)      197120      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 50, 128)      512         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 50, 128)      16512       batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 383,648\n",
      "Trainable params: 383,136\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(3342, 50, 128) (3342, 4) (3342, 50, 128)\n",
      "Epoch 1/500\n",
      "3342/3342 [==============================] - 23s 7ms/step - loss: 0.1555 - categorical_accuracy: 0.0371A: 19 - ETA: 1s - loss: 0.1602 - categorical_ac\n",
      "Epoch 2/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0794 - categorical_accuracy: 0.0490A: 17s - loss: 0.0783 - categ\n",
      "Epoch 3/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0788 - categorical_accuracy: 0.0492\n",
      "Epoch 4/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0778 - categorical_accuracy: 0.0556\n",
      "Epoch 5/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0772 - categorical_accuracy: 0.0589 9s - loss:\n",
      "Epoch 6/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0768 - categorical_accuracy: 0.0635\n",
      "Epoch 7/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0766 - categorical_accuracy: 0.0635\n",
      "Epoch 8/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0761 - categorical_accuracy: 0.0661\n",
      "Epoch 9/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0760 - categorical_accuracy: 0.0627\n",
      "Epoch 10/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0758 - categorical_accuracy: 0.0632 1s - loss: 0.0758 - categorica\n",
      "Epoch 11/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0755 - categorical_accuracy: 0.0644\n",
      "Epoch 12/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0753 - categorical_accuracy: 0.0711A: 15s - loss: 0.0766 - categoric - ETA: 11s - loss: 0.0765 - categorical_accuracy: 0 - ETA: 10s \n",
      "Epoch 13/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0753 - categorical_accuracy: 0.0668\n",
      "Epoch 14/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0767 - categorical_accuracy: 0.0589\n",
      "Epoch 15/500\n",
      "3342/3342 [==============================] - 22s 6ms/step - loss: 0.0762 - categorical_accuracy: 0.0609 1s - loss: 0.0763 - categorical_ac\n",
      "Epoch 16/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0759 - categorical_accuracy: 0.0613\n",
      "Epoch 17/500\n",
      "3342/3342 [==============================] - ETA: 0s - loss: 0.0759 - categorical_accuracy: 0.0641 - ETA: 12s - loss: 0.0752 - categorical_accuracy: 0 - ETA: 11s - loss: 0.0754 - categorical_accuracy: 0.06 - ETA:  - 21s 6ms/step - loss: 0.0759 - categorical_accuracy: 0.0640\n",
      "Epoch 18/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0757 - categorical_accuracy: 0.0620\n",
      "Epoch 19/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0756 - categorical_accuracy: 0.0634\n",
      "Epoch 20/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0755 - categorical_accuracy: 0.0652\n",
      "Epoch 21/500\n",
      "3342/3342 [==============================] - ETA: 0s - loss: 0.0756 - categorical_accuracy: 0.06 - 21s 6ms/step - loss: 0.0756 - categorical_accuracy: 0.0672\n",
      "Epoch 22/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0755 - categorical_accuracy: 0.0669\n",
      "Epoch 23/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0754 - categorical_accuracy: 0.0668\n",
      "Epoch 24/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0753 - categorical_accuracy: 0.0680\n",
      "Epoch 25/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0753 - categorical_accuracy: 0.0663\n",
      "Epoch 26/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0753 - categorical_accuracy: 0.0688 0s - loss: 0.0753 - categorical_accuracy\n",
      "Epoch 27/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0750 - categorical_accuracy: 0.0709\n",
      "Epoch 28/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0749 - categorical_accuracy: 0.0684 8s - loss: 0 -\n",
      "Epoch 29/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0749 - categorical_accuracy: 0.0693A: 12s - loss: 0.0753 - categorical_accuracy: - ETA: 4s - loss: 0.0747 - categorical_accuracy: 0. - ETA: 4s -\n",
      "Epoch 30/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0748 - categorical_accuracy: 0.0705\n",
      "Epoch 31/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0747 - categorical_accuracy: 0.0698 3s - loss: 0.0753 - catego - ETA: 1s - loss: 0.0749 - categorica\n",
      "Epoch 32/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0745 - categorical_accuracy: 0.0735A: 15s - loss: 0.0 - ETA: 3s - loss: 0\n",
      "Epoch 33/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0746 - categorical_accuracy: 0.0697\n",
      "Epoch 34/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0744 - categorical_accuracy: 0.0733\n",
      "Epoch 35/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0744 - categorical_accuracy: 0.0698 0s - loss: 0.0744 - categorical_accuracy: 0.06\n",
      "Epoch 36/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0744 - categorical_accuracy: 0.0705\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0743 - categorical_accuracy: 0.0713\n",
      "Epoch 38/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0741 - categorical_accuracy: 0.0729 4s - loss: 0.0740 - categorical_accura - ETA: 3s - loss: 0.074\n",
      "Epoch 39/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0741 - categorical_accuracy: 0.0719\n",
      "Epoch 40/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0742 - categorical_accuracy: 0.0727A: 19s - loss: 0.0\n",
      "Epoch 41/500\n",
      "3342/3342 [==============================] - 26s 8ms/step - loss: 0.0740 - categorical_accuracy: 0.0740\n",
      "Epoch 42/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0738 - categorical_accuracy: 0.0723\n",
      "Epoch 43/500\n",
      "3342/3342 [==============================] - 27s 8ms/step - loss: 0.0738 - categorical_accuracy: 0.0725\n",
      "Epoch 44/500\n",
      "3342/3342 [==============================] - ETA: 0s - loss: 0.0739 - categorical_accuracy: 0.0723 ETA: 9s - loss: 0.0736  - ETA:  - ETA: 1s - loss: 0.0741 - categorica - 21s 6ms/step - loss: 0.0739 - categorical_accuracy: 0.0724\n",
      "Epoch 45/500\n",
      "3342/3342 [==============================] - 27s 8ms/step - loss: 0.0737 - categorical_accuracy: 0.0720\n",
      "Epoch 46/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0736 - categorical_accuracy: 0.0752A: 16s - loss: 0.0737 - categorical_accura - ETA: 14s - loss: 0.0743 - categorical_ - E\n",
      "Epoch 47/500\n",
      "3342/3342 [==============================] - 28s 8ms/step - loss: 0.0736 - categorical_accuracy: 0.0746\n",
      "Epoch 48/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0735 - categorical_accuracy: 0.0726\n",
      "Epoch 49/500\n",
      "3342/3342 [==============================] - 25s 7ms/step - loss: 0.0735 - categorical_accuracy: 0.0731A: 12s - loss: 0.0738 - categorical_a - ETA: 9s - loss: 0.0740 - categorical_accuracy - ETA: 7s - loss: 0.073 - ETA: 3s - loss: 0.0734 - ca - ETA: 0s - loss: 0.0735 - categorical_accuracy: \n",
      "Epoch 50/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0734 - categorical_accuracy: 0.0746\n",
      "Epoch 51/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0735 - categorical_accuracy: 0.0729\n",
      "Epoch 52/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0733 - categorical_accuracy: 0.0765\n",
      "Epoch 53/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0732 - categorical_accuracy: 0.0732\n",
      "Epoch 54/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0732 - categorical_accuracy: 0.0753\n",
      "Epoch 55/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0731 - categorical_accuracy: 0.0736\n",
      "Epoch 56/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0731 - categorical_accuracy: 0.0748\n",
      "Epoch 57/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0730 - categorical_accuracy: 0.0742A: 17 - ETA: 9s - loss: 0\n",
      "Epoch 58/500\n",
      "3342/3342 [==============================] - 25s 7ms/step - loss: 0.0739 - categorical_accuracy: 0.0720\n",
      "Epoch 59/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0733 - categorical_accuracy: 0.0736A: 10s \n",
      "Epoch 60/500\n",
      "3342/3342 [==============================] - 24s 7ms/step - loss: 0.0731 - categorical_accuracy: 0.0725\n",
      "Epoch 61/500\n",
      "3342/3342 [==============================] - 25s 8ms/step - loss: 0.0730 - categorical_accuracy: 0.0758\n",
      "Epoch 62/500\n",
      "3342/3342 [==============================] - 22s 7ms/step - loss: 0.0731 - categorical_accuracy: 0.0733A: 18s - loss: 0.0741 - categorical_accuracy - ETA: 16s - loss: 0.0740 - categorical_accurac - ETA: 13s - loss: 0.0740 - categorical_accuracy: 0.070 - ETA: \n",
      "Epoch 63/500\n",
      "3342/3342 [==============================] - 26s 8ms/step - loss: 0.0731 - categorical_accuracy: 0.0756\n",
      "Epoch 64/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0729 - categorical_accuracy: 0.0729\n",
      "Epoch 65/500\n",
      "3342/3342 [==============================] - 27s 8ms/step - loss: 0.0729 - categorical_accuracy: 0.0730\n",
      "Epoch 66/500\n",
      "3342/3342 [==============================] - 21s 6ms/step - loss: 0.0728 - categorical_accuracy: 0.0753A: 18s - loss: 0.0 - ETA: 11s - loss: 0.07 - ETA: 7s - - ETA: 2s - loss: 0.0728 \n",
      "Epoch 67/500\n",
      "3342/3342 [==============================] - 27s 8ms/step - loss: 0.0728 - categorical_accuracy: 0.0740 5s - loss:\n",
      "Epoch 68/500\n",
      "1696/3342 [==============>...............] - ETA: 10s - loss: 0.0729 - categorical_accuracy: 0.0753- ETA: 17s - loss: 0"
     ]
    }
   ],
   "source": [
    "# Want to penalize each output node independantly. \n",
    "# Log Loss aka multi-class multi-label as sigmoid -> binary CE, as want probs to be considered independent of each other.\n",
    "# Combo of sigmoid and crossentropy here log counteracts exp to reduce the saturation :)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam, # consider changing this one for others\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(time()))\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=0, mode=\"auto\")\n",
    "\n",
    "print(train_xs.shape, train_composers.shape, train_ys.shape)\n",
    "model.fit([train_xs, train_composers], train_ys,\n",
    "          epochs=500, # Train harder more for more things was too bad train man :(\n",
    "          batch_size=32,\n",
    "          shuffle=True, # shuffle here but not when constructing set to be able to validate later on :)\n",
    "          #callbacks=[tensorboard],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict([train_xs, train_composers], verbose=True)\n",
    "maxes = [np.max(c) for c in a]\n",
    "plt.hist(maxes)\n",
    "plt.show()\n",
    "plt.hist(a[:,-1])\n",
    "plt.show()\n",
    "b = np.max(a[1][-1])\n",
    "plt.plot(a[1][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[0].T, fs=5)\n",
    "prep.visualize_piano_roll(train_xs[0].T, fs=5)\n",
    "plt.plot(a[100][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[100].T, fs=5)\n",
    "prep.visualize_piano_roll(train_xs[100].T, fs=5)\n",
    "plt.plot(a[200][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[200].T, fs=5)\n",
    "prep.visualize_piano_roll(train_xs[200].T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/nostate_50_offset_10_specialist_regularizer.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_song_from_predict(model, initial_data, composer, limit):\n",
    "    song = []\n",
    "    keep_producing = True\n",
    "    prev_data = initial_data\n",
    "    while keep_producing and len(song) < limit:\n",
    "        predictions = model.predict([np.array([prev_data]), composer])[0]\n",
    "        labels = np.zeros(predictions[-1].shape)\n",
    "        labels[predictions[-1]/np.max(predictions[-1])>0.75] = 1 # Threshold to consider the key as active, binarized based on this\n",
    "        last_output = labels\n",
    "        keep_producing = np.sum(last_output) != len(last_output)\n",
    "        song.append(last_output)\n",
    "        prev_data = np.append(prev_data[1:], [last_output], 0)\n",
    "    return np.array(song)\n",
    "\n",
    "initial_step = 543\n",
    "steps = 10\n",
    "song1 = make_song_from_predict(model, train_xs[initial_step], np.array([[1.0, 0.0, 0.0, 0.0]]), sequence_length*steps)\n",
    "song2 = make_song_from_predict(model, train_xs[initial_step], np.array([[0.0, 1.0, 0.0, 0.0]]), sequence_length*steps)\n",
    "song3 = make_song_from_predict(model, train_xs[initial_step], np.array([[0.0, 0.0, 1.0, 0.0]]), sequence_length*steps)\n",
    "song4 = make_song_from_predict(model, train_xs[initial_step], np.array([[0.0, 0.0, 0.0, 1.0]]), sequence_length*steps)\n",
    "prep.embed_play_v1(song1.T, fs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.embed_play_v1(song2.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.embed_play_v1(song3.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.embed_play_v1(song4.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.visualize_piano_roll(song1.T, fs=5)\n",
    "prep.visualize_piano_roll(song2.T, fs=5)\n",
    "prep.visualize_piano_roll(song3.T, fs=5)\n",
    "prep.visualize_piano_roll(song4.T, fs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
