{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import LSTM, Dropout, Dense, BatchNormalization, Activation, Input, TimeDistributed, Masking\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU, PReLU\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta, Adagrad\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "from assignment.helpers import datapreparation as prep\n",
    "\n",
    "\n",
    "# Can make the midi actually play by multiply the notes (0-128, so 1 is basically silence).\n",
    "\n",
    "# Note on specifying the initial state of RNNs\n",
    "# Seems as though you can reset state (probably stateful=True), and then pass an array of initial states that can be used\n",
    "# in the RNN - so initialize based on composer :D\n",
    "# https://keras.io/layers/recurrent/\n",
    "\n",
    "fs1_dirpath = \"./assignment/datasets/training/piano_roll_fs1\"\n",
    "\n",
    "datasets = prep.load_all_dataset(fs1_dirpath)\n",
    "dataset_names = prep.load_all_dataset_names(fs1_dirpath)\n",
    "\n",
    "datasets = [dataset[:, 1:] for dataset in datasets] # Remove the headers\n",
    "\n",
    "dataset_id_names = dict(zip(np.arange(len(dataset_names)), dataset_names))\n",
    "longest_song = max(datasets[i].shape[1] for i in range(len(datasets)))\n",
    "sequence_length = 10\n",
    "number_of_batches = longest_song//sequence_length + 1\n",
    "num_keys = len(datasets[0])\n",
    "pad_length = number_of_batches*sequence_length\n",
    "num_songs = len(datasets)\n",
    "\n",
    "def preprocess_for_stateful_with_padding(dataset, num_songs, sequence_length, num_batches, num_keys, pad_length):\n",
    "    big_af = [[[] for a in range(num_songs)] for b in range(num_batches)]\n",
    "    songs_padded = pad_sequences(dataset, maxlen=pad_length, padding=\"post\", value=np.array([-1.0 for _ in range(num_keys)]))\n",
    "    for i in range(num_batches):\n",
    "        for j in range(num_songs):\n",
    "            big_af[i][j] = songs_padded[j, i*sequence_length:(i+1)*sequence_length]\n",
    "    return np.array(big_af)\n",
    "\n",
    "\n",
    "datasets = np.array([dataset.T for dataset in datasets])\n",
    "xs = preprocess_for_stateful_with_padding(datasets, num_songs, sequence_length, number_of_batches, num_keys, pad_length)\n",
    "datasets_labels = np.array([np.append(dataset[1:,:], np.array([np.ones(num_keys)]), axis=0) for dataset in datasets])\n",
    "ys = preprocess_for_stateful_with_padding(datasets, num_songs, sequence_length, number_of_batches, num_keys, pad_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(batch_shape=(number_of_batches, num_songs, sequence_length, num_keys))\n",
    "#mask = Masking(mask_value=-1.0)(inputs)\n",
    "# Units = units per timestep LSTM block, i.e. output dimensionality (128 here since input and output 128 keys)\n",
    "lstm1 = LSTM(num_keys,\n",
    "               activation='relu',\n",
    "               return_sequences=True,\n",
    "               stateful=True,\n",
    "               dropout=0.0, #0.2, #0.25,\n",
    "               recurrent_dropout=0.0, #0.25,\n",
    "               kernel_regularizer=None,#l2(0.0001),\n",
    "               recurrent_regularizer=None, #l2(0.0001),\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,#l2(0.0001),\n",
    "               )(inputs)\n",
    "normalized1 = BatchNormalization()(lstm1)\n",
    "dense1 = Dense(num_keys, activation=\"sigmoid\")(normalized1)\n",
    "lstm2 = LSTM(num_keys,\n",
    "               activation='relu',\n",
    "               return_sequences=True,\n",
    "               stateful=True,\n",
    "               dropout=0.0, #0.2, #0.25,\n",
    "               recurrent_dropout=0.0, #0.25,\n",
    "               kernel_regularizer=None,#l2(0.0001),\n",
    "               recurrent_regularizer=None, #l2(0.0001),\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,#l2(0.0001),\n",
    "               )(dense1)\n",
    "normalized2 = BatchNormalization()(lstm2)\n",
    "dense2 = Dense(num_keys, activation=\"sigmoid\")(normalized2)\n",
    "outputs = TimeDistributed(Dense(num_keys, activation=\"sigmoid\"))(normalized2) # Sigmoid keeps the probabilities independent of each other, while softmax does not!\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "rmsprop = RMSprop(lr=0.001)\n",
    "adagrad =  Adagrad(lr=0.001)\n",
    "adam = Adam(lr=0.001, amsgrad=True) #Ends up in a point where gradients really small, denominator really small and then loss exploding\n",
    "adadelta = Adadelta(lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (84, 10, 128)             0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (84, 10, 128)             131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (84, 10, 128)             512       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (84, 10, 128)             16512     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (84, 10, 128)             131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (84, 10, 128)             512       \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (84, 10, 128)             16512     \n",
      "=================================================================\n",
      "Total params: 297,216\n",
      "Trainable params: 296,704\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_12 to have 3 dimensions, but got array with shape (84, 43, 10, 128)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-5f503eb334f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_of_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m           )\n",
      "\u001b[1;32mc:\\users\\hanak\\venv\\music\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hanak\\venv\\music\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hanak\\venv\\music\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_12 to have 3 dimensions, but got array with shape (84, 43, 10, 128)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Want to penalize each output node independantly. So we pick a binary loss \n",
    "# and model the output of the network as a independent bernoulli distributions per label.\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam, # consider changing this one for others\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(time()))\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=0, mode=\"auto\")\n",
    "\n",
    "\n",
    "model.fit(xs, ys,\n",
    "          epochs=250, # Train harder more for more things was too bad train man :(\n",
    "          batch_size=number_of_batches,\n",
    "          shuffle=False,\n",
    "          callbacks=[tensorboard],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xs_split' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-e51b9648e09c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmaxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xs_split' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "a = model.predict(xs_split, verbose=True)\n",
    "print(model.layers[1].states[0])\n",
    "print(model.layers[1].states[0])\n",
    "maxes = [np.max(c) for c in a]\n",
    "plt.hist(maxes)\n",
    "plt.show()\n",
    "plt.hist(a[:,-1])\n",
    "plt.show()\n",
    "b = np.max(a[1][-1])\n",
    "plt.plot(a[0][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[0].T, fs=1)\n",
    "prep.visualize_piano_roll(xs_split[0].T, fs=1)\n",
    "plt.plot(a[100][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[100].T, fs=1)\n",
    "prep.visualize_piano_roll(xs_split[100].T, fs=1)\n",
    "plt.plot(a[200][-1])\n",
    "plt.show()\n",
    "prep.visualize_piano_roll(a[200].T, fs=1)\n",
    "prep.visualize_piano_roll(xs_split[200].T, fs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_xs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d425ba840c32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0minitial_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1250\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_song_from_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minitial_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_piano_roll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_piano_roll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minitial_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_xs' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def make_song_from_predict(model, initial_data, limit):\n",
    "    song = []\n",
    "    keep_producing = True\n",
    "    prev_data = initial_data\n",
    "    while keep_producing and len(song) < limit:\n",
    "        #print(\"input\", prev_data)\n",
    "        predictions = model.predict(np.array([prev_data]))[0]\n",
    "        #print(\"output\", predictions[-1])\n",
    "        #plt.plot(predictions[-1])\n",
    "        #plt.show()\n",
    "        labels = np.zeros(predictions.shape)\n",
    "        labels[predictions>0.5] = 1 # Weak activations, want to scale to 0/1\n",
    "        last_output = labels[-1]\n",
    "        #print(\"output scaled:\", last_output)\n",
    "        keep_producing = np.sum(last_output) != len(last_output)\n",
    "        song.append(last_output)\n",
    "        prev_data = np.append(prev_data[1:], [last_output], 0)\n",
    "    return np.array(song)\n",
    "\n",
    "initial_step = 1250\n",
    "song = make_song_from_predict(model, test_xs[initial_step], sequence_length)\n",
    "prep.visualize_piano_roll(song.T, fs=1)\n",
    "prep.visualize_piano_roll(test_xs[initial_step].T, fs=1)\n",
    "prep.embed_play_v1(song.T, fs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_xs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-580333857d69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_piano_roll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minitial_step\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_play_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minitial_step\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_xs' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "prep.visualize_piano_roll(test_xs[initial_step+1].T, fs=1)\n",
    "prep.embed_play_v1(test_xs[initial_step+1].T, fs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
